<!-- ormd:0.1 -->
---
title: "Adaptive Reward System - Decision Tree"
authors: ["Emergence Engine Team"]
dates:
  created: '2025-11-11T05:40:38.726861Z'
links: []
status: "complete"
description: "Emergence Engine documentation"
---

# ğŸŒ³ Adaptive Reward System - Decision Tree

## Start Here: Should You Implement This?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Are resource-seeking weights weak?     â”‚
â”‚ (scores < 0.1 after 20+ generations)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”œâ”€ YES â†’ Continue â†“
            â”‚
            â””â”€ NO â†’ You're fine, skip this
            
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Do agents learn survival but not       â”‚
â”‚ resource collection?                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”œâ”€ YES â†’ Continue â†“
            â”‚
            â””â”€ NO â†’ Different problem, debug first

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Is current reward (~6 Ï‡) smaller than  â”‚
â”‚ metabolic cost of typical search?      â”‚
â”‚ (cost â‰ˆ 0.4 Ï‡/s Ã— find_time)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”œâ”€ YES â†’ âœ… IMPLEMENT ADAPTIVE REWARDS
            â”‚
            â””â”€ NO â†’ Just increase CONFIG.rewardChi

```

---

## Implementation Approach: Which Option?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Choose Your Reward Strategy             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                                 â”‚
            â–¼                                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ OPTION 1:       â”‚           â”‚ OPTION 2:       â”‚
    â”‚ Adaptive/       â”‚           â”‚ Absolute        â”‚
    â”‚ Behavioral      â”‚  â†PICK    â”‚ Biological      â”‚
    â”‚                 â”‚   THIS!   â”‚ Anchor          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                              â”‚
             â–¼                              â–¼
    Pro:                          Pro:
    â€¢ Auto-scales                 â€¢ Biologically exact
    â€¢ RL-friendly                 â€¢ Principled
    â€¢ Adapts to difficulty        
                                   Con:
    Con:                          â€¢ Very high rewards
    â€¢ Needs tuning                â€¢ May dominate learning
    â€¢ One extra parameter         â€¢ Fixed value
    
    USE CASE:                     USE CASE:
    Normal training,              Research/modeling
    want good learning            exact metabolism
```

**Recommendation:** **Option 1 (Adaptive/Behavioral)** with biological comments for context.

---

## Configuration Strategy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ How aggressive should rewards be?       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚             â”‚             â”‚             â”‚
            â–¼             â–¼             â–¼             â–¼
    Conservative   Balanced      Generous    Extreme
    gainFactor=4   gainFactor=6  gainFactor=8  gainFactor=10
    
    â€¢ Safer       â€¢ Recommended  â€¢ Faster      â€¢ High risk
    â€¢ Slower      â€¢ Good balance â€¢ learning     â€¢ May exploit
    â€¢ Stable      â€¢ ~3x boost    â€¢ ~4x boost   â€¢ ~5x boost
    
    rewardâ‰ˆ12Ï‡    rewardâ‰ˆ19Ï‡    rewardâ‰ˆ25Ï‡    rewardâ‰ˆ32Ï‡
```

**Recommendation:** Start with **gainFactor=6**, increase if learning is still slow.

---

## EMA Tuning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ How fast should rewards adapt?          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚             â”‚             â”‚             â”‚
            â–¼             â–¼             â–¼             â–¼
    Very Slow      Slow        Medium      Fast
    alpha=0.05    alpha=0.1   alpha=0.15  alpha=0.2
    
    â€¢ Stable      â€¢ Recommended â€¢ Responsive â€¢ Volatile
    â€¢ Ignores     â€¢ Smooths     â€¢ Tracks     â€¢ Noisy
      outliers      noise         trends    
    â€¢ ~20 samples â€¢ ~10 samples â€¢ ~7 samples â€¢ ~5 samples
      to converge   to converge   to converge  to converge
```

**Recommendation:** Start with **alpha=0.1**, decrease if rewards are too noisy.

---

## Implementation Phases

```
                    START
                      â”‚
                      â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ PHASE 1: Core Tracking (30 min)    â”‚
    â”‚ â€¢ Add EMA to World                  â”‚
    â”‚ â€¢ Add config settings               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼ [Test: EMA updates]
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ PHASE 2: Calculation (30 min)      â”‚
    â”‚ â€¢ Create reward functions           â”‚
    â”‚ â€¢ Add safety bounds                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼ [Test: Math correct]
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ PHASE 3: Integration (1 hour)      â”‚
    â”‚ â€¢ Hook into collection              â”‚
    â”‚ â€¢ Scale learning signals            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼ [Test: Full episode]
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ PHASE 4: Validation (1 hour)       â”‚
    â”‚ â€¢ Run 10 training generations       â”‚
    â”‚ â€¢ Compare with baseline             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼ [Good results?]
                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ NO                      â”‚ YES
    â–¼                         â–¼
    Tune parameters     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    (adjust gainFactor) â”‚ PHASE 5: Polish      â”‚
    â”‚                   â”‚ â€¢ Add HUD display    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â†’          â”‚ â€¢ Enhance features   â”‚
                        â”‚ â€¢ Document results   â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
                               DONE âœ…
```

---

## Troubleshooting Decision Tree

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Is resource-seeking improving?          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”œâ”€ YES â†’ âœ… Success! Keep training
            â”‚
            â”œâ”€ NO â†’ Check avgFindTime in HUD
            â”‚        â”‚
            â”‚        â”œâ”€ <5s â†’ Resources too easy
            â”‚        â”‚        â€¢ Add more agents
            â”‚        â”‚        â€¢ Larger environment
            â”‚        â”‚
            â”‚        â”œâ”€ 5-15s â†’ Normal range
            â”‚        â”‚          Check reward value
            â”‚        â”‚          â”‚
            â”‚        â”‚          â”œâ”€ <10Ï‡ â†’ Increase gainFactor
            â”‚        â”‚          â”œâ”€ 10-30Ï‡ â†’ Good, keep training
            â”‚        â”‚          â””â”€ >50Ï‡ â†’ Decrease gainFactor
            â”‚        â”‚
            â”‚        â””â”€ >20s â†’ Resources too hard
            â”‚                  â€¢ Check if agents are stuck
            â”‚                  â€¢ Reduce environment size
            â”‚                  â€¢ Check wall penalties

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Are rewards stable?                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”œâ”€ YES â†’ âœ… Good
            â”‚
            â””â”€ NO (jumping wildly)
                      â”‚
                      â”œâ”€ Decrease emaAlpha (0.1â†’0.05)
                      â”œâ”€ Check for bugs (NaN, Infinity)
                      â””â”€ Verify safety bounds active

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Is training too slow?                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”œâ”€ Increase gainFactor (6â†’8)
            â”œâ”€ Decrease evapPerSec (make trails less valuable)
            â”œâ”€ Decrease residualGainPerSec (make reuse less rewarding)
            â””â”€ Check collectResource multiplier in Phase 6
```

---

## Quick Reference Card

### Key Formulas

```javascript
// Effective metabolic cost
C_base = baseDecay + moveFraction Ã— moveCost
       = 0.15 + 0.7 Ã— 0.35
       = 0.395 Ï‡/s

// Adaptive reward
reward = gainFactor Ã— C_base Ã— avgFindTime

// EMA update
avgFindTime = (1 - alpha) Ã— oldAvg + alpha Ã— currentFindTime

// With defaults:
reward = 6 Ã— 0.395 Ã— 8 = 18.96 Ï‡ â‰ˆ 19 Ï‡
```

### Default Configuration

```javascript
adaptiveReward: {
  enabled: true,
  gainFactor: 6.0,          // 4-10 range
  avgMoveFraction: 0.7,     // 70% moving
  emaAlpha: 0.1,            // Slow smoothing
  minReward: 3.0,           // Safety floor
  maxReward: 100.0,         // Safety ceiling
}
```

### Expected Reward Ranges

| Find Time | Conservative (4x) | Balanced (6x) | Generous (8x) |
|-----------|-------------------|---------------|---------------|
| 5s        | 8 Ï‡              | 12 Ï‡         | 16 Ï‡         |
| 8s        | 13 Ï‡             | 19 Ï‡         | 25 Ï‡         |
| 12s       | 19 Ï‡             | 28 Ï‡         | 38 Ï‡         |
| 15s       | 24 Ï‡             | 36 Ï‡         | 47 Ï‡         |
| 20s       | 32 Ï‡             | 47 Ï‡         | 63 Ï‡         |

---

## Files to Modify

```
Priority 1 (Core):
â”œâ”€â”€ config.js          [+20 lines] Add adaptiveReward config
â”œâ”€â”€ rewards.js         [+40 lines] Add calculation functions
â”œâ”€â”€ app.js             [+30 lines] Integrate & track EMA
â””â”€â”€ app.js (World)     [+10 lines] Add EMA state

Priority 2 (Optional):
â”œâ”€â”€ app.js (HUD)       [+10 lines] Display stats
â””â”€â”€ rewards.js         [+5 lines]  Scale provenance credit

Documentation:
â”œâ”€â”€ REWARD_SYSTEM_IMPLEMENTATION_PLAN.md [Created âœ…]
â”œâ”€â”€ REWARD_SYSTEM_SUMMARY.md             [Created âœ…]
â””â”€â”€ REWARD_DECISION_TREE.md              [This file âœ…]
```

---

## Success Checklist

### After Implementation
- [ ] Code compiles without errors
- [ ] HUD shows avgFindTime and reward
- [ ] No NaN or Infinity values
- [ ] Rewards in expected range (10-40 Ï‡)

### After 10 Generations
- [ ] avgFindTime has stabilized
- [ ] Rewards are consistent
- [ ] No crashes or performance issues
- [ ] Best reward still improving

### After 20 Generations
- [ ] Resource score > 0.10 (was 0.04)
- [ ] Turnâ†’resDx/resDy weights > 0.1
- [ ] Thrustâ†’resVis weight > 0.1
- [ ] Compare favorably with Gen 30 baseline

### After 30-40 Generations
- [ ] Resource score > 0.15
- [ ] Seeking weights > 0.2
- [ ] Agents visibly seek resources
- [ ] Best reward > 60

---

## The One-Page Summary

**Problem:** Fixed 6 Ï‡ reward too weak â†’ agents don't learn to seek resources

**Solution:** Dynamic reward scaling with search difficulty
```
reward = 6 Ã— metabolic_cost_per_sec Ã— avg_time_to_find
       â‰ˆ 19 Ï‡ (for typical 8-second searches)
```

**Implementation:** 4 phases, ~4 hours, ~100 lines total

**Risk:** Low (backward compatible, bounded, toggleable)

**Reward:** 3-5x stronger learning signal â†’ resource-seeking in ~20 gens

**Next Step:** Implement Phase 1-3, test, then continue based on results

---

**Ready to code? Start with `REWARD_SYSTEM_IMPLEMENTATION_PLAN.md` Phase 1!** ğŸš€

